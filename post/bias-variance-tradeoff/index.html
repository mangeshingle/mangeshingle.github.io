<!DOCTYPE html>
<html lang="en-us">

<head>
  <meta charset="UTF-8">
  <title>
    Bias-Variance Tradeoff (Geometric Intuition) | Mangesh Ingle
  </title>
  <meta name="viewport" content="width=device-width,user-scalable=no,maximum-scale=1,initial-scale=1">
  <link rel="canonical" href="index.html" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans">
  <link rel="stylesheet" href="../../css/sanitize.css">
  <link rel="stylesheet" href="../../css/responsive.css">
  <link rel="stylesheet" href="../../css/highlight.css">
  <link rel="stylesheet" href="../../css/theme.css">
  <link rel="stylesheet" href="../../css/custom.css">
  <link href="../../index.xml" rel="alternate" type="application/rss+xml" title="Mangesh Ingle" />
  <link href="../../index.xml" rel="feed" type="application/rss+xml" title="Mangesh Ingle" />
</head>

<body>
  <div class="container">
    <header role="banner">
      <div class="row gutters">
        <div id="site-title" class="col span_6">
          <h1><a href="../../index.html">Mangesh Ingle</a></h1>
          <h2>Data Explorer & Machine Learning Enthusiast</h2>
        </div>
        <div id="social" class="col span_6">
          <ul>
            <li><a href="about/index.html">About</a></li>
            <li><a href="post/index.html">Archive</a></li>
            <li><a href="https://github.com/mangeshingle" target="_blank">Github</a></li>
            <li><a href="https://www.linkedin.com/in/mangesh-ingle-b53122176/" target="_blank">LinkedIn</a></li>
          </ul>
        </div>
      </div>
    </header>



    <main id="single" role="main">
      <div class="article-header">
        <h1>Bias-Variance Tradeoff (Geometric Intuition)</h1>
        <div class="meta">
          Nov 26, 2018 &nbsp;
        </div>
      </div>
      <article>


        <p><a href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff">Bias-Variance Tradeoff</a>
          decomposition is a way of analyzing a learning algorithm's
          expected generalization error with respect to a particular problem as a sum of three terms,
          the bias, variance, and a quantity called the irreducible error, resulting from noise in the problem itself.</p>

        <p>
          Before understanding bias-variance tradeoff, we need to understand the concept of overfitting and
          underfitting.
          Overfitting and underfitting are very closely related to bias and variance terms, but mind that, both the
          concepts are
          equally important to understand 'generalization error'(Unseen data).
        </p>

        <h2 id="content">Content</h2>
        <ol>
          <li>
            <a href="#over-under">How overfitting and underfitting works?</a>
          </li>
          <li>
            <a href="#bias-variance">What is bias and variance?</a>
          </li>
          <li>
            <a href="#bias-variance-tradeoff">Understanding bias-variance tradeoff</a>Further Reading
          </li>
          <li>
            <a href="#reference">Further Reading</a>
          </li>
        </ol>


        <h2 name="over-under" id="over-under">How overfitting and underfitting works?</h2>
        <p>
          We can explain overfitting and underfitting concepts using KNN(K-Nearest Neighbour) model.
          As the value of 'k' changes, model starts to fall in the overfit or underfit zone.
          How it works, is explained in the following image.
        </p>
        <img src="../../images/overfitting_underfitting.jpg" alt="over-under" width="800" height="450">
        <p>
          As we can see in the above image,when k=1, we get overfit model, because decision boundary considers each and
          every point in the data.When k=n, we starts to underfit as model starts to favour the class which has
          majority in the dataset.
          We must always find the decision boundary between overfit and underfit zone,(here it is k=5),which is also
          considered as a well fit model
        </p>
        <p>
          We understood the geometric intuition behind the overfitting and underfitting concept.
          Now lets understand the few cocepts like bias and variance.
        </p>

        <h2 name="bias-variance" id="bias-variance">What is bias and variance?</h2>
        <p>
          To understand the mathematical basis, we have a concept like 'bias-variance tradeoff', which comes from the
          field of
          statistical/theoratical machine learning.<br /><br />
          <b>Bias(underfitting): Set of assumptions that the learner uses to predict outputs, given inputs that it has
            not encountered.</b><br /><br />
          In machine learning, one aims to construct algorithms that are able to learn to predict a certain target
          output.
          To achieve this, the learning algorithm is presented some training examples that demonstrate the intended
          relation of input and output values.
          Then the learner is supposed to approximate the correct output, even for examples that have not been shown
          during training.
          Without any additional assumptions, this problem cannot be solved exactly since unseen situations might have
          an arbitrary output value.
          The kind of necessary assumptions about the nature of the target function are subsumed in the phrase
          'inductive bias'.<br /><br />
          <b>Variance(overfitting): An error from sensitivity to small fluctuations in the training set.</b><br /><br />
          Small changes in training dataset results in very different model(decision surfaces) and this large changes
          in the model
          is generally referred as 'high-variance model'.
        </p>
        <p>
          We understood the concept of bias and variance,now lets understand our main concept of 'bias-variance
          tradeoff'
        </p>

        <h2 name="bias-variance-tradeoff" id="bias-variance-tradeoff">Understanding bias-variance tradeoff</h2>
        <p>
          <b>The mathematical formula for 'bias-variance tradeoff': </b><br />
          <p>
            <img src="../../images/bias-varince-tradeoff.jpg" alt="over-under" width="800" height="200">
          </p>
        </p>
        <p>
          <b>Generalization Error: </b>Future unseen data(like, using test dataset to evaluate a model).<br />
          <b>Bias: </b>Error due to simplifying assumptions.<br />
          <b>Variance: </b>Error caused by model due to slight change in training data.<br />
          <b>Irreducible Error: </b>Error that can't reduce further.<br />
        </p>
        <p>
          We must reduce the generalization error to get unbiased low variance model.
          And to achive that, we should reduce the bias and variance in the model.<br /><br />
          Now lets understand the bias term first,by assuming that we have 80% -ve points and 20% +ve points in our
          training dataset
          <p>
            <img src="../../images/gen-err.jpg" alt="over-under" width="500" height="300">
          </p>
          Here we can easily see that non linear(curve) decision boundary(boundary-1) can easily separate -ve points
          form +ve points.
          But suppose we made the simplifying model assumption that our model will use line or plane(boundary-2 or
          boundary-3)
          to separate -ve points from +ve points.<br /><br />
          We can see that, if we use boundary-2 and boundary-3 as a decision surface then we will misclassify most of
          the points as wrong class labels.
        </p>
        <p>
          Our training dataset is highly imbalanced towards -ve points, then for every query point, model will predict
          -ve points as a resultant class label.(that means model is underfitting)
          <b>That means due to simplifying assumption, our model will become 'high bias model'.</b><br />
          We understood the high bias concept, now lets undestand how high variance affects our model to overfit.
        </p>
        <p>
          Now lets understand the variance term.Random splitting is performed two times and KNN model is trained with
          k=1
          <p>
            <img src="../../images/high-var.jpg" alt="over-under" width="800" height="500">
          </p>
          From figure, we can evaluate that small changes in training dataset result in very different model(decision
          surface) and
          this large changes in the model is generally referred as 'high variance model'(that means model is
          overfitting).
        </p>
        <p>
          *** Values are assumed to understand the concept better,(bias + variance + irreducible error)<br />
          k = 1 , 10 + 100 + 3 ---> 113 (Overfit)<br />
          k = 5 , 12 + 10 + 3 ---> 25 (Well fit)<br />
          k = 1 , 100 + 2 + 3 ---> 105 (Underfit)<br />
        </p>

        <p>
          So at the end we conclude that,there is no escaping the relationship between bias and variance in machine
          learning.<br />
          ~~> Increasing the bias will decrease the variance.<br />
          ~~> Increasing the variance will decrease the bias.<br /><br />

          There is a trade-off at play between these two concerns and the algorithms you choose and the way you choose
          to configure them are finding different balances in this trade-off for your problem.
          <p>
            In reality, we cannot calculate the real bias and variance error terms because we do not know the actual
            underlying target function. Nevertheless, as a framework, bias and variance provide the tools to understand
            the behavior of machine learning algorithms in the pursuit of predictive performance.
          </p>
        </p>
        <h2 name="reference" id="reference">Further Reading</h2>
        <p>
          This section lists some recommend resources if you are looking to learn more about bias, variance and the
          bias-variance trade-off.
          <ul>
            <li><a href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff">wikipedia</a></li>
            <li><a href="https://en.wikipedia.org/wiki/Inductive_bias">wikipedia</a></li>
            <li><a href="https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/bias-variance-tradeoff-3/">Applied ai course</a></li>
            <li><a href="https://www.youtube.com/watch?v=zrEyxfl2-a8&list=PLD63A284B7615313A&index=8">youtube</a></li>
          </ul>
        </p>
      </article>


      <aside>
        <div id="disqus_thread"></div>
      </aside>

      <script type="text/javascript">

        var disqus_shortname = 'mangeshingle';
        (function () {
          var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
          dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
          (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
      </script>
      <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by
          Disqus.</a></noscript>




    </main>

    <nav class="pagination-single">
      <center>
        <a href="#">Back to Top</a>
      </center>

    </nav>

    <footer role="contentinfo">
      <div style="text-align:center; font-size: 0.8em">
        Copyright Â© 2018 Mangesh Ingle. All Rights Reserved.
      </div>
    </footer>


  </div>

  <script src="../../js/jquery-3.2.1.min.js"></script>
  <script src="../../js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
  <script src="../../../cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJaxdda6.js?config=TeX-AMS-MML_HTMLorMML">
  </script>

  <script>
    $(document).ready(function () {
      var links = document.querySelectorAll('article a');
      for (var i = 0, length = links.length; i < length; i++) {
        if (links[i].hostname != window.location.hostname) {
          links[i].target = '_blank';
        }
      }
    });
  </script>

  <script>
    (function (i, s, o, g, r, a, m) {
      i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
        (i[r].q = i[r].q || []).push(arguments)
      }, i[r].l = 1 * new Date(); a = s.createElement(o),
        m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
    })(window, document, 'script', '../../../www.google-analytics.com/analytics.js', 'ga');
    ga('create', 'UA-30554548-1', 'auto');
    ga('send', 'pageview');
  </script>

</body>

</html>